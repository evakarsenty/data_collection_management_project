{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8a745e-1d40-4db9-8a6c-d5f6d6100a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, size, array_remove, expr, array_contains, array, lit, arrays_zip,  when, first, udf, concat_ws, to_json, flatten\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import BertEmbeddings, Tokenizer\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import BooleanType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9b740e-2034-4b0a-8c81-8af315e56332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49aec8c-bce3-4736-a8e1-acf572284a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from torch) (2.11.3)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.9/site-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.8.0)\r\n",
      "Requirement already satisfied: fsspec in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: sympy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: networkx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-963881cb-6062-4da1-8192-b156820fad86/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552cca53-57e3-4aa7-8e67-80ed59534a4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e289bba-5c27-486c-9b40-43d608f59536",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "profiles = spark.read.parquet('/linkedin/people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff7caa9-7591-4222-9a8b-910269bd7e5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Israeli_Profiles = pd.read_csv(\"/dbfs/FileStore/shared_uploads/raph@campus.technion.ac.il/Israeli_Profiles-2.csv\")\n",
    "Israeli_Profiles = Israeli_Profiles.dropna(subset=['current_company_company_id'])\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "israeli_profiles = spark.createDataFrame(Israeli_Profiles)\n",
    "\n",
    "deleted_columns = ['url', 'locale', 'name', 'avatar', 'followers', 'connections', 'posts',\n",
    "                   'recommendations', 'canonical_url', 'publications', 'locations', 'country_code',\n",
    "                   'input_url', 'warning', 'warning_code', 'error', 'current_company']\n",
    "\n",
    "Israeli_Profiles_clean = israeli_profiles.drop(*deleted_columns)\n",
    "\n",
    "\n",
    "def extract_info_udf(json_str, info_type):\n",
    "    if json_str is None:\n",
    "        return None\n",
    "\n",
    "    extracted_info = []\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if info_type == \"projects\":\n",
    "                    title = item.get('title')\n",
    "                    duration = item.get('duration')\n",
    "                    description = item.get('description')\n",
    "                    extracted_info.append(f\"{title} ({duration}): {description}\")\n",
    "                elif info_type == \"patents\":\n",
    "                    title = item.get('title')\n",
    "                    extracted_info.append(f\"{title}\")\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "# Register the UDF\n",
    "extract_info = F.udf(lambda json_str, info_type: extract_info_udf(json_str, info_type), returnType=ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the DataFrame for projects\n",
    "extracted_info_df = Israeli_Profiles_clean.withColumn(\n",
    "    \"project_info\", extract_info(\"projects\", F.lit(\"projects\"))\n",
    ")\n",
    "\n",
    "# Apply the UDF to the DataFrame for patents\n",
    "extracted_info_df = extracted_info_df.withColumn(\n",
    "    \"patent_info\", extract_info(\"patents\", F.lit(\"patents\"))\n",
    ")\n",
    "\n",
    "# Add IDs to DataFrames\n",
    "Israeli_Profiles_clean_with_id = Israeli_Profiles_clean.withColumn(\"id\", monotonically_increasing_id())\n",
    "final_info_df_with_id = extracted_info_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Join DataFrames using the unique identifiers\n",
    "joined_df = Israeli_Profiles_clean_with_id.join(final_info_df_with_id.select(\"id\", \"project_info\", \"patent_info\"), \"id\", \"left_outer\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "final_israeli_joined_df = joined_df.drop(\"id\", \"projects\", \"patents\").filter(col(\"current_company_company_id\").isNotNull())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4345a247-0522-4f30-a862-029f15f4eb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanProfiles = profiles.select(\"about\", \n",
    "                        col(\"certifications.title\").alias(\"certifications_title\"), \n",
    "                        \"city\", \n",
    "                        col(\"current_company.industry\").alias(\"current_company_industry\"), \n",
    "                        col(\"current_company.title\").alias(\"current_company_title\"), \n",
    "                        col(\"current_company.company_id\").alias(\"current_company_id\"), \n",
    "                        col(\"current_company.name\").alias(\"current_company_name\"), \n",
    "                        \"education.degree\", \n",
    "                        \"education.field\", \n",
    "                        col(\"experience.company\").alias(\"experience_company\"), \n",
    "                        col(\"experience.description\").alias(\"experience_description\"), \n",
    "                        col(\"experience.duration\").alias(\"experience_duration\"), \n",
    "                        col(\"experience.positions.title\").alias(\"positions_title\"), \n",
    "                        col(\"experience.title\").alias(\"experience_title\"), \n",
    "                        \"id\", \n",
    "                        col(\"languages.subtitle\").alias(\"languages_level\"), \n",
    "                        col(\"languages.title\").alias(\"languages\"), \n",
    "                        \"position\", \n",
    "                        \"recommendations_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27329ef-b2c7-4c4c-8406-52a7739d66a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we need to remove all the rows in cleanProfiles that dont have lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2c8dd6-32ef-4f44-bfa4-f06fecdfaa91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def contains_only_null(arr):\n",
    "    if arr is None:\n",
    "        return False\n",
    "    for val in arr:\n",
    "        if val is not None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Register UDF\n",
    "contains_only_null_udf = udf(contains_only_null, BooleanType())\n",
    "\n",
    "# Filtering condition\n",
    "filtered_df = cleanProfiles.filter(\n",
    "    (col(\"current_company_id\").isNotNull()) |\n",
    "    (~contains_only_null_udf(\"experience_company\"))\n",
    ")\n",
    "remove_nulls = filtered_df.withColumn(\"experience_company\", expr(\"filter(experience_company, x -> x is not null)\"))\n",
    "filteredProfiles = remove_nulls.filter(size(col(\"experience_company\")) != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ae417f-1e1b-4c31-86d4-58f2136f217c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we need to add a label column. a label is:\n",
    "\n",
    "the current company\n",
    "  if the profile doesnt have a current company, themn it is its previues company ( data is cleaned in a way so at least one is guranteed )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949159d0-d5ef-42c1-b8d3-a2ed6d46f3fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def combine_columns(current_company_id, experience_company):\n",
    "    if current_company_id is not None:\n",
    "        return current_company_id\n",
    "    elif experience_company is not None and len(experience_company) > 0:\n",
    "        return experience_company[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Add new column using UDF\n",
    "readyProfiles = filteredProfiles.withColumn(\"label\", combine_columns(filteredProfiles.current_company_id, filteredProfiles.experience_company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d020ab47-88e3-4f81-a40d-c503c5ae2751",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = readyProfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e517dc0c-a31f-46f4-aa1e-5f884591e60a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"certifications_title\", concat_ws(\";\", col(\"certifications_title\"))) \\\n",
    "                   .withColumn(\"degree\", concat_ws(\";\", col(\"degree\"))) \\\n",
    "                   .withColumn(\"field\", concat_ws(\";\", col(\"field\"))) \\\n",
    "                   .withColumn(\"experience_company\", concat_ws(\";\", col(\"experience_company\"))) \\\n",
    "                   .withColumn(\"experience_description\", concat_ws(\";\", col(\"experience_description\"))) \\\n",
    "                   .withColumn(\"experience_duration\", concat_ws(\";\", col(\"experience_duration\"))) \\\n",
    "                   .withColumn(\"positions_title\", concat_ws(\";\", flatten(col(\"positions_title\")))) \\\n",
    "                   .withColumn(\"experience_title\", concat_ws(\";\", col(\"experience_title\"))) \\\n",
    "                   .withColumn(\"languages_level\", concat_ws(\";\", col(\"languages_level\"))) \\\n",
    "                   .withColumn(\"languages\", concat_ws(\";\", col(\"languages\")))\n",
    "\n",
    "# Converting array columns to string columns separated by ';'\n",
    "final_israeli_joined_df = final_israeli_joined_df.withColumn(\"region\", concat_ws(\";\", col(\"region\"))) \\\n",
    "                   .withColumn(\"position\", concat_ws(\";\", col(\"position\"))) \\\n",
    "                   .withColumn(\"educations_details\", concat_ws(\";\", col(\"educations_details\"))) \\\n",
    "                   .withColumn(\"recommendations_count\", concat_ws(\";\", col(\"recommendations_count\"))) \\\n",
    "                   .withColumn(\"current_company_name\", concat_ws(\";\", col(\"current_company_name\"))) \\\n",
    "                   .withColumn(\"current_company_company_id\", concat_ws(\";\", col(\"current_company_company_id\"))) \\\n",
    "                   .withColumn(\"project_info\", concat_ws(\";\", col(\"project_info\"))) \\\n",
    "                   .withColumn(\"patent_info\", concat_ws(\";\", col(\"patent_info\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816d121e-6761-4ae8-9fa8-8eb0c5f2fd5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def combine_features(*values):\n",
    "    return ' ;'.join(map(str, values))\n",
    "\n",
    "def mean_vector(embeddings):\n",
    "    words_embeddings = [word.embeddings for word in embeddings]\n",
    "    embeddings_array = np.array(words_embeddings)\n",
    "    if embeddings_array.size == 0:\n",
    "        return Vectors.dense([])\n",
    "    mean_embedding = np.mean(embeddings_array, axis=0)\n",
    "    return Vectors.dense(mean_embedding.tolist())\n",
    "\n",
    "mean_vector_udf = udf(mean_vector, VectorUDT())\n",
    "concat_udf = udf(combine_features, StringType())\n",
    "\n",
    "columns_to_combine = [col for col in df.columns if col != \"label\"]\n",
    "columns_to_combine_israeli = [col for col in final_israeli_joined_df.columns if col != \"label\"]\n",
    "\n",
    "final_israeli_joined_df = final_israeli_joined_df.withColumn('label', concat_udf('current_company_company_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ff05f5-27b4-42e1-8f6f-9e3d6a0fc1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_israeli_joined_df = final_israeli_joined_df.withColumn('concatenated_text', concat_udf(*columns_to_combine_israeli))\n",
    "df = final_israeli_joined_df.withColumn('concatenated_label', concat_udf('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c225e310-c559-4c3f-baf5-a89cfa96fb6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\n",
      "Approximate size to download 16.1 MB\n",
      "\r[ | ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"concatenated_text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings.pretrained(name=\"small_bert_L2_128\", lang=\"en\")\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\\\n",
    "    .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631306a3-c4d8-4fa6-8983-6724122dc985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([document_assembler, tokenizer, bert_embeddings])\n",
    "pipeline_israeli = Pipeline().setStages([document_assembler, tokenizer, bert_embeddings])\n",
    "\n",
    "fitted_pipeline_israeli = pipeline_israeli.fit(final_israeli_joined_df)\n",
    "fitted_pipeline = pipeline.fit(df)\n",
    "\n",
    "processed_features = fitted_pipeline.transform(df)\n",
    "processed_features_israeli = fitted_pipeline_israeli.transform(final_israeli_joined_df)\n",
    "\n",
    "final = processed_features.withColumn(\"mean_embeddings\", mean_vector_udf(\"embeddings\"))\n",
    "final_israeli = processed_features_israeli.withColumn(\"mean_embeddings\", mean_vector_udf(\"embeddings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05ba647-5f01-472f-a2dc-24fd58924ebb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "readydf = final.select(col(\"label\"), col(\"mean_embeddings\").alias(\"features\"))\n",
    "readydf_israeli = final_israeli.select(col(\"label\"), col(\"mean_embeddings\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2008c09b-66dd-499e-80a0-c50f7834b864",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "readydf = readydf.withColumn('features', vector_to_array('features'))\n",
    "readydf_israeli = readydf_israeli.withColumn('features', vector_to_array('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc469668-20bc-4bbb-b5fa-f8298f73fd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"label\")\n",
    "df_with_count = readydf.withColumn(\"label_count\", F.count(\"label\").over(windowSpec))\n",
    "df_with_count_israeli = readydf_israeli.withColumn(\"label_count\", F.count(\"label\").over(windowSpec))\n",
    "\n",
    "# Now, filter the DataFrame to keep only rows with labels that occur more than once\n",
    "filtered_df = df_with_count.filter(col(\"label_count\") > 4).drop(\"label_count\")\n",
    "filtered_df_israeli = df_with_count_israeli.filter(col(\"label_count\") > 4).drop(\"label_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5915327f-c597-4d93-ba71-3b135c5b18dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|         label|            features|\n",
      "+--------------+--------------------+\n",
      "|  4m-analytics|[-1.6014958847136...|\n",
      "|  4m-analytics|[-0.7555547985045...|\n",
      "|  4m-analytics|[-1.1667294127068...|\n",
      "|  4m-analytics|[-1.5575048347999...|\n",
      "|  4m-analytics|[-1.1995340954210...|\n",
      "|  4m-analytics|[-1.0348859432118...|\n",
      "|  4m-analytics|[-1.8541329440317...|\n",
      "|access-fintech|[-0.8120101155506...|\n",
      "|access-fintech|[-1.1172558730840...|\n",
      "|access-fintech|[-1.2331138528883...|\n",
      "|access-fintech|[-1.6496962183400...|\n",
      "|access-fintech|[-1.1008056953097...|\n",
      "|access-fintech|[-1.1813397212670...|\n",
      "|      akeyless|[-2.1274398490786...|\n",
      "|      akeyless|[-1.3856773419039...|\n",
      "|      akeyless|[-1.5513129610082...|\n",
      "|      akeyless|[-2.0918836268511...|\n",
      "|      akeyless|[-1.6197486480077...|\n",
      "|      akeyless|[-1.8969496453509...|\n",
      "|      aleph-vc|[-0.8357561141615...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df_israeli.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df93e0f-79e6-4af2-96e0-a7d9cdb8a78e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_israeli = filtered_df_israeli.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b7ceb6-f97b-4c79-b3be-1256263de8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4m-analytics</td>\n",
       "      <td>[-1.6014958847136724, -0.5573168041716728, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4m-analytics</td>\n",
       "      <td>[-0.7555547985045806, 0.3190507866928111, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4m-analytics</td>\n",
       "      <td>[-1.1667294127068348, -0.9156654881579536, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4m-analytics</td>\n",
       "      <td>[-1.557504834799931, -0.0739575208582241, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4m-analytics</td>\n",
       "      <td>[-1.1995340954210307, -0.38181648342996033, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>zone7ai</td>\n",
       "      <td>[-1.1773088642142036, -0.6362856812775135, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>zone7ai</td>\n",
       "      <td>[-1.5285623893141747, -0.039684190725286804, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>zone7ai</td>\n",
       "      <td>[-1.360643278219198, -1.0163621925993969, -0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>zone7ai</td>\n",
       "      <td>[-1.7080026865005493, -0.3067014639576276, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>zone7ai</td>\n",
       "      <td>[-1.6834647845138202, -0.46056046235290443, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4m-analytics</td>\n      <td>[-1.6014958847136724, -0.5573168041716728, -0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4m-analytics</td>\n      <td>[-0.7555547985045806, 0.3190507866928111, -0.0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4m-analytics</td>\n      <td>[-1.1667294127068348, -0.9156654881579536, -0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4m-analytics</td>\n      <td>[-1.557504834799931, -0.0739575208582241, -0.4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4m-analytics</td>\n      <td>[-1.1995340954210307, -0.38181648342996033, -0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1092</th>\n      <td>zone7ai</td>\n      <td>[-1.1773088642142036, -0.6362856812775135, -0....</td>\n    </tr>\n    <tr>\n      <th>1093</th>\n      <td>zone7ai</td>\n      <td>[-1.5285623893141747, -0.039684190725286804, -...</td>\n    </tr>\n    <tr>\n      <th>1094</th>\n      <td>zone7ai</td>\n      <td>[-1.360643278219198, -1.0163621925993969, -0.7...</td>\n    </tr>\n    <tr>\n      <th>1095</th>\n      <td>zone7ai</td>\n      <td>[-1.7080026865005493, -0.3067014639576276, -0....</td>\n    </tr>\n    <tr>\n      <th>1096</th>\n      <td>zone7ai</td>\n      <td>[-1.6834647845138202, -0.46056046235290443, -0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1097 rows × 2 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf_israeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2fa9e6-2d31-486f-8601-56b35d391124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = filtered_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca1050e-3511-4878-9767-08480b600ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Only the scraping data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746bc947-6342-4e95-957a-7d108c204bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_israeli = pdf_israeli.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e21669-28b4-4947-b6d6-8030e0edf679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = pdf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c2c56d-6681-4282-b3d1-89e24fba188d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3f7dbe-e175-447e-a2df-1c674eb345d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UniqueLabels = pandas_df_israeli['label'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(UniqueLabels)}\n",
    "pandas_df_israeli['label'] = pandas_df_israeli['label'].map(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7250696-7671-4f2a-b4ba-02ef13e3857d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and labels (y)\n",
    "X = np.array(pandas_df_israeli['features'].tolist())\n",
    "y = np.array(pandas_df_israeli['label'])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135b04bb-abf6-49f2-9bb2-0517877030bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: distance | K: 5 | Accuracy: 0.55 | Precision: 0.6543939393939394 | Recall: 0.55 | F1-score: 0.5580952380952381\n",
      "Weights: distance | K: 7 | Accuracy: 0.55 | Precision: 0.6678030303030302 | Recall: 0.55 | F1-score: 0.5627272727272727\n",
      "Weights: distance | K: 9 | Accuracy: 0.5272727272727272 | Precision: 0.655530303030303 | Recall: 0.5272727272727272 | F1-score: 0.5417099567099567\n",
      "Weights: distance | K: 11 | Accuracy: 0.5181818181818182 | Precision: 0.6442640692640693 | Recall: 0.5181818181818182 | F1-score: 0.5297979797979797\n",
      "Weights: distance | K: 13 | Accuracy: 0.5363636363636364 | Precision: 0.645909090909091 | Recall: 0.5363636363636364 | F1-score: 0.5409848484848485\n",
      "Weights: distance | K: 15 | Accuracy: 0.55 | Precision: 0.6678030303030303 | Recall: 0.55 | F1-score: 0.5568398268398268\n",
      "Weights: distance | K: 17 | Accuracy: 0.5636363636363636 | Precision: 0.6521212121212121 | Recall: 0.5636363636363636 | F1-score: 0.5637012987012987\n",
      "Weights: distance | K: 19 | Accuracy: 0.5590909090909091 | Precision: 0.6406060606060606 | Recall: 0.5590909090909091 | F1-score: 0.5556493506493506\n",
      "Weights: distance | K: 21 | Accuracy: 0.55 | Precision: 0.6118181818181818 | Recall: 0.55 | F1-score: 0.5403463203463204\n",
      "Weights: distance | K: 23 | Accuracy: 0.5590909090909091 | Precision: 0.6221969696969697 | Recall: 0.5590909090909091 | F1-score: 0.5461688311688311\n",
      "Weights: distance | K: 25 | Accuracy: 0.5409090909090909 | Precision: 0.6057575757575758 | Recall: 0.5409090909090909 | F1-score: 0.528073593073593\n",
      "Weights: distance | K: 27 | Accuracy: 0.5454545454545454 | Precision: 0.6128787878787878 | Recall: 0.5454545454545454 | F1-score: 0.5346536796536797\n",
      "Weights: distance | K: 29 | Accuracy: 0.5272727272727272 | Precision: 0.5754545454545454 | Recall: 0.5272727272727272 | F1-score: 0.5108008658008657\n",
      "Weights: distance | K: 31 | Accuracy: 0.5045454545454545 | Precision: 0.5465909090909091 | Recall: 0.5045454545454545 | F1-score: 0.4868831168831168\n",
      "Weights: distance | K: 33 | Accuracy: 0.4954545454545455 | Precision: 0.5462121212121211 | Recall: 0.4954545454545455 | F1-score: 0.4771645021645022\n",
      "Weights: distance | K: 35 | Accuracy: 0.5045454545454545 | Precision: 0.5575757575757575 | Recall: 0.5045454545454545 | F1-score: 0.48686147186147183\n",
      "Weights: distance | K: 37 | Accuracy: 0.5045454545454545 | Precision: 0.5636363636363636 | Recall: 0.5045454545454545 | F1-score: 0.48883116883116884\n",
      "Weights: distance | K: 39 | Accuracy: 0.509090909090909 | Precision: 0.576590909090909 | Recall: 0.509090909090909 | F1-score: 0.4958658008658009\n",
      "Weights: distance | K: 41 | Accuracy: 0.5 | Precision: 0.5568939393939395 | Recall: 0.5 | F1-score: 0.48253246753246753\n",
      "Weights: distance | K: 43 | Accuracy: 0.4909090909090909 | Precision: 0.5463852813852814 | Recall: 0.4909090909090909 | F1-score: 0.4721356421356421\n",
      "Weights: distance | K: 45 | Accuracy: 0.4727272727272727 | Precision: 0.528961038961039 | Recall: 0.4727272727272727 | F1-score: 0.45215728715728715\n",
      "Weights: distance | K: 47 | Accuracy: 0.4681818181818182 | Precision: 0.5175974025974026 | Recall: 0.4681818181818182 | F1-score: 0.4426551226551227\n",
      "Weights: distance | K: 49 | Accuracy: 0.4681818181818182 | Precision: 0.4998701298701298 | Recall: 0.4681818181818182 | F1-score: 0.4387157287157287\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initializing the K-Nearest Neighbors classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=19, weights=\"distance\")\n",
    "\n",
    "# Fitting the classifier to the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting labels for the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy, precision, recall, f1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted'), recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Printing evaluation metrics\n",
    "print(\"Weights:\", w, \"| K:\", k, \"| Accuracy:\", accuracy, \"| Precision:\", precision, \"| Recall:\", recall, \"| F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49accf5-3366-497c-bf67-8f59ed0f7c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Without Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa666f9e-58e2-48ea-b7f4-df60f4f01cbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033e0b40-7241-4603-806e-3a89942a1b46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = pdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4504e4ea-ed36-4246-a44b-f17d9958ecf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UniqueLabels = pandas_df['label'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(UniqueLabels)}\n",
    "pandas_df['label'] = pandas_df['label'].map(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1d11e0-b064-4a7e-aa47-57837a1dd0b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and labels (y)\n",
    "X = np.array(pandas_df['features'].tolist())\n",
    "y = np.array(pandas_df['label'])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dab22cdf-6fa7-4b30-967c-c16983220ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b073adef-7f8f-4960-91e8-7faf1e0e57e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: distance | K: 1 | Accuracy: 0.17710077000334784 | Precision: 0.19505024174569763 | Recall: 0.17710077000334784 | F1-score: 0.1762038970665142\n",
      "Weights: distance | K: 3 | Accuracy: 0.18316873116839638 | Precision: 0.19892416889085393 | Recall: 0.18316873116839638 | F1-score: 0.18087511767690456\n",
      "Weights: distance | K: 5 | Accuracy: 0.1896342484097757 | Precision: 0.20063819938946664 | Recall: 0.1896342484097757 | F1-score: 0.18457918394139647\n",
      "Weights: distance | K: 7 | Accuracy: 0.19354703716103114 | Precision: 0.20030099460226883 | Recall: 0.19354703716103114 | F1-score: 0.18570727568625967\n",
      "Weights: distance | K: 9 | Accuracy: 0.1965391697355206 | Precision: 0.1991862988615002 | Recall: 0.1965391697355206 | F1-score: 0.18596968089839128\n",
      "Weights: distance | K: 11 | Accuracy: 0.19884081017743555 | Precision: 0.19761923979821464 | Recall: 0.19884081017743555 | F1-score: 0.1854544781958786\n",
      "Weights: distance | K: 13 | Accuracy: 0.2006611985269501 | Precision: 0.19585214453843852 | Recall: 0.2006611985269501 | F1-score: 0.1847199629612948\n",
      "Weights: distance | K: 15 | Accuracy: 0.20210495480415133 | Precision: 0.19511692326154098 | Recall: 0.20210495480415133 | F1-score: 0.18411975365487643\n",
      "Weights: distance | K: 17 | Accuracy: 0.20356963508536993 | Precision: 0.19454980485486534 | Recall: 0.20356963508536993 | F1-score: 0.18353149500713392\n",
      "Weights: distance | K: 19 | Accuracy: 0.20390441914964846 | Precision: 0.19330892154308998 | Recall: 0.20390441914964846 | F1-score: 0.18200335733681278\n",
      "Weights: distance | K: 21 | Accuracy: 0.2046158352862404 | Precision: 0.19268819517535268 | Recall: 0.2046158352862404 | F1-score: 0.18105168297587582\n",
      "Weights: distance | K: 23 | Accuracy: 0.20499246735855373 | Precision: 0.19160206544591707 | Recall: 0.20499246735855373 | F1-score: 0.1799538727529307\n",
      "Weights: distance | K: 25 | Accuracy: 0.20532725142283229 | Precision: 0.19076786967835127 | Recall: 0.20532725142283229 | F1-score: 0.1789697163947774\n",
      "Weights: distance | K: 27 | Accuracy: 0.20543187144291933 | Precision: 0.19009317423658903 | Recall: 0.20543187144291933 | F1-score: 0.17785500182444522\n",
      "Weights: distance | K: 29 | Accuracy: 0.205159859390693 | Precision: 0.18931875078022875 | Recall: 0.205159859390693 | F1-score: 0.17631958213133167\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing the K-Nearest Neighbors classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=19, weights=\"distance\")\n",
    "\n",
    "# Fitting the classifier to the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting labels for the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy, precision, recall, f1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted'), recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Printing evaluation metrics\n",
    "print(\"Weights:\", w, \"| K:\", k, \"| Accuracy:\", accuracy, \"| Precision:\", precision, \"| Recall:\", recall, \"| F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae39fa0-dd79-4bc8-bf78-7f37c6f62ca1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e76ca29-6efd-4abd-aba5-c34593dec315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = pdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81819e28-7e24-4787-8f16-21068f667dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ComplexNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.75):\n",
    "        super(ComplexNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        nn.Tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = np.array(self.df.iloc[idx]['features'], dtype=np.float32)\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return features, label\n",
    "\n",
    "UniqueLabels = pandas_df['label'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(UniqueLabels)}\n",
    "pandas_df['label'] = pandas_df['label'].map(label_to_idx)\n",
    "\n",
    "outputLayer = len(UniqueLabels)\n",
    "# Hyperparameters\n",
    "input_size = 128\n",
    "hidden_size1 = 512\n",
    "hidden_size2 = 512\n",
    "# hidden_size2 = int(round((128)*(2/3)+len(UniqueLabels)))\n",
    "output_size = len(UniqueLabels)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 75\n",
    "batch_size = 64\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "grouped_df = pandas_df.groupby('label')\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for label, group in grouped_df:\n",
    "    train_group, test_group = train_test_split(group, test_size=0.2, random_state=42)\n",
    "    train_data.append(train_group)\n",
    "    test_data.append(test_group)\n",
    "\n",
    "train_df = pd.concat(train_data)\n",
    "test_df = pd.concat(test_data)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Create dataset and dataloader for train and test sets\n",
    "train_dataset = CustomDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_df)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model_nn_noScraping = ComplexNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_nn_noScraping.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c21e49e-a661-4577-910f-39325b1dc55f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Train Loss: 8.2259, Test Loss: 7.1025, Top-3 Test Accuracy: 0.1895, took: 171.44365859031677 seconds\n",
      "Epoch [2/75], Train Loss: 7.1658, Test Loss: 6.4797, Top-3 Test Accuracy: 0.2450, took: 171.76789903640747 seconds\n",
      "Epoch [3/75], Train Loss: 6.7542, Test Loss: 6.1575, Top-3 Test Accuracy: 0.2791, took: 170.70981407165527 seconds\n",
      "Epoch [4/75], Train Loss: 6.5515, Test Loss: 5.9895, Top-3 Test Accuracy: 0.2971, took: 224.8121018409729 seconds\n",
      "Epoch [5/75], Train Loss: 6.4214, Test Loss: 5.8613, Top-3 Test Accuracy: 0.3121, took: 171.80496883392334 seconds\n",
      "Epoch [6/75], Train Loss: 6.3273, Test Loss: 5.7694, Top-3 Test Accuracy: 0.3216, took: 231.53384733200073 seconds\n",
      "Epoch [7/75], Train Loss: 6.2577, Test Loss: 5.7005, Top-3 Test Accuracy: 0.3292, took: 171.94415187835693 seconds\n",
      "Epoch [8/75], Train Loss: 6.2069, Test Loss: 5.6578, Top-3 Test Accuracy: 0.3346, took: 171.91447734832764 seconds\n",
      "Epoch [9/75], Train Loss: 6.1602, Test Loss: 5.6082, Top-3 Test Accuracy: 0.3403, took: 172.82059001922607 seconds\n",
      "Epoch [10/75], Train Loss: 6.1251, Test Loss: 5.5805, Top-3 Test Accuracy: 0.3443, took: 171.3566334247589 seconds\n",
      "Epoch [11/75], Train Loss: 6.0920, Test Loss: 5.5510, Top-3 Test Accuracy: 0.3473, took: 171.93359231948853 seconds\n",
      "Epoch [12/75], Train Loss: 6.0670, Test Loss: 5.5206, Top-3 Test Accuracy: 0.3509, took: 226.58115577697754 seconds\n",
      "Epoch [13/75], Train Loss: 6.0453, Test Loss: 5.5028, Top-3 Test Accuracy: 0.3525, took: 171.7982075214386 seconds\n",
      "Epoch [14/75], Train Loss: 6.0187, Test Loss: 5.4607, Top-3 Test Accuracy: 0.3549, took: 221.65225553512573 seconds\n",
      "Epoch [15/75], Train Loss: 5.9970, Test Loss: 5.4526, Top-3 Test Accuracy: 0.3567, took: 229.1460793018341 seconds\n",
      "Epoch [16/75], Train Loss: 5.9841, Test Loss: 5.4386, Top-3 Test Accuracy: 0.3580, took: 168.86091685295105 seconds\n",
      "Epoch [17/75], Train Loss: 5.9667, Test Loss: 5.4348, Top-3 Test Accuracy: 0.3592, took: 173.8821985721588 seconds\n",
      "Epoch [18/75], Train Loss: 5.9550, Test Loss: 5.3929, Top-3 Test Accuracy: 0.3645, took: 231.36416697502136 seconds\n",
      "Epoch [19/75], Train Loss: 5.9309, Test Loss: 5.3820, Top-3 Test Accuracy: 0.3659, took: 231.31982731819153 seconds\n",
      "Epoch [20/75], Train Loss: 5.9230, Test Loss: 5.3693, Top-3 Test Accuracy: 0.3671, took: 223.03515338897705 seconds\n",
      "Epoch [21/75], Train Loss: 5.9063, Test Loss: 5.3574, Top-3 Test Accuracy: 0.3661, took: 221.52220225334167 seconds\n",
      "Epoch [22/75], Train Loss: 5.8903, Test Loss: 5.3362, Top-3 Test Accuracy: 0.3690, took: 171.8500189781189 seconds\n",
      "Epoch [23/75], Train Loss: 5.8882, Test Loss: 5.3252, Top-3 Test Accuracy: 0.3715, took: 227.21565103530884 seconds\n",
      "Epoch [24/75], Train Loss: 5.8634, Test Loss: 5.3186, Top-3 Test Accuracy: 0.3714, took: 173.52189421653748 seconds\n",
      "Epoch [25/75], Train Loss: 5.8615, Test Loss: 5.2961, Top-3 Test Accuracy: 0.3737, took: 225.50417613983154 seconds\n",
      "Epoch [26/75], Train Loss: 5.8519, Test Loss: 5.2863, Top-3 Test Accuracy: 0.3745, took: 172.1357626914978 seconds\n",
      "Epoch [27/75], Train Loss: 5.8412, Test Loss: 5.2875, Top-3 Test Accuracy: 0.3739, took: 175.60662722587585 seconds\n",
      "Epoch [28/75], Train Loss: 5.8302, Test Loss: 5.2880, Top-3 Test Accuracy: 0.3743, took: 227.41639852523804 seconds\n",
      "Epoch [29/75], Train Loss: 5.8228, Test Loss: 5.2691, Top-3 Test Accuracy: 0.3767, took: 172.47446417808533 seconds\n",
      "Epoch [30/75], Train Loss: 5.8145, Test Loss: 5.2472, Top-3 Test Accuracy: 0.3783, took: 171.93272876739502 seconds\n",
      "Epoch [31/75], Train Loss: 5.8066, Test Loss: 5.2590, Top-3 Test Accuracy: 0.3794, took: 233.14427661895752 seconds\n",
      "Epoch [32/75], Train Loss: 5.7968, Test Loss: 5.2468, Top-3 Test Accuracy: 0.3787, took: 172.03952860832214 seconds\n",
      "Epoch [33/75], Train Loss: 5.7969, Test Loss: 5.2292, Top-3 Test Accuracy: 0.3814, took: 172.00844717025757 seconds\n",
      "Epoch [34/75], Train Loss: 5.7920, Test Loss: 5.2149, Top-3 Test Accuracy: 0.3828, took: 173.84749031066895 seconds\n",
      "Epoch [35/75], Train Loss: 5.7719, Test Loss: 5.2093, Top-3 Test Accuracy: 0.3828, took: 172.73794960975647 seconds\n",
      "Epoch [36/75], Train Loss: 5.7736, Test Loss: 5.2097, Top-3 Test Accuracy: 0.3829, took: 232.1283037662506 seconds\n",
      "Epoch [37/75], Train Loss: 5.7673, Test Loss: 5.1925, Top-3 Test Accuracy: 0.3848, took: 172.34239292144775 seconds\n",
      "Epoch [38/75], Train Loss: 5.7590, Test Loss: 5.1903, Top-3 Test Accuracy: 0.3860, took: 229.49679350852966 seconds\n",
      "Epoch [39/75], Train Loss: 5.7524, Test Loss: 5.1760, Top-3 Test Accuracy: 0.3872, took: 171.13437724113464 seconds\n",
      "Epoch [40/75], Train Loss: 5.7466, Test Loss: 5.1735, Top-3 Test Accuracy: 0.3867, took: 178.7880802154541 seconds\n",
      "Epoch [41/75], Train Loss: 5.7513, Test Loss: 5.1755, Top-3 Test Accuracy: 0.3864, took: 228.98813605308533 seconds\n",
      "Epoch [42/75], Train Loss: 5.7362, Test Loss: 5.1615, Top-3 Test Accuracy: 0.3877, took: 171.6621448993683 seconds\n",
      "Epoch [43/75], Train Loss: 5.7274, Test Loss: 5.1555, Top-3 Test Accuracy: 0.3889, took: 170.89998316764832 seconds\n",
      "Epoch [44/75], Train Loss: 5.7331, Test Loss: 5.1485, Top-3 Test Accuracy: 0.3895, took: 232.0605490207672 seconds\n",
      "Epoch [45/75], Train Loss: 5.7190, Test Loss: 5.1416, Top-3 Test Accuracy: 0.3926, took: 226.98177528381348 seconds\n",
      "Epoch [46/75], Train Loss: 5.7172, Test Loss: 5.1362, Top-3 Test Accuracy: 0.3903, took: 173.9456126689911 seconds\n",
      "Epoch [47/75], Train Loss: 5.7142, Test Loss: 5.1303, Top-3 Test Accuracy: 0.3929, took: 230.15849089622498 seconds\n",
      "Epoch [48/75], Train Loss: 5.7092, Test Loss: 5.1403, Top-3 Test Accuracy: 0.3913, took: 174.15056467056274 seconds\n",
      "Epoch [49/75], Train Loss: 5.7051, Test Loss: 5.1209, Top-3 Test Accuracy: 0.3936, took: 172.24365162849426 seconds\n",
      "Epoch [50/75], Train Loss: 5.7013, Test Loss: 5.1219, Top-3 Test Accuracy: 0.3921, took: 172.37856340408325 seconds\n",
      "Epoch [51/75], Train Loss: 5.6959, Test Loss: 5.1216, Top-3 Test Accuracy: 0.3930, took: 170.0818989276886 seconds\n",
      "Epoch [52/75], Train Loss: 5.6916, Test Loss: 5.1291, Top-3 Test Accuracy: 0.3911, took: 230.34859013557434 seconds\n",
      "Epoch [53/75], Train Loss: 5.6903, Test Loss: 5.1178, Top-3 Test Accuracy: 0.3927, took: 228.68553972244263 seconds\n",
      "Epoch [54/75], Train Loss: 5.6846, Test Loss: 5.1116, Top-3 Test Accuracy: 0.3926, took: 212.8919632434845 seconds\n",
      "Epoch [55/75], Train Loss: 5.6852, Test Loss: 5.1081, Top-3 Test Accuracy: 0.3931, took: 175.3579044342041 seconds\n",
      "Epoch [56/75], Train Loss: 5.6738, Test Loss: 5.0963, Top-3 Test Accuracy: 0.3954, took: 222.2786180973053 seconds\n",
      "Epoch [57/75], Train Loss: 5.6790, Test Loss: 5.0897, Top-3 Test Accuracy: 0.3956, took: 173.0972182750702 seconds\n",
      "Epoch [58/75], Train Loss: 5.6696, Test Loss: 5.0884, Top-3 Test Accuracy: 0.3985, took: 229.5599501132965 seconds\n",
      "Epoch [59/75], Train Loss: 5.6764, Test Loss: 5.0858, Top-3 Test Accuracy: 0.3979, took: 231.93418908119202 seconds\n",
      "Epoch [60/75], Train Loss: 5.6706, Test Loss: 5.0993, Top-3 Test Accuracy: 0.3958, took: 174.96569156646729 seconds\n",
      "Epoch [61/75], Train Loss: 5.6587, Test Loss: 5.0851, Top-3 Test Accuracy: 0.3978, took: 173.5330832004547 seconds\n",
      "Epoch [62/75], Train Loss: 5.6641, Test Loss: 5.0696, Top-3 Test Accuracy: 0.3991, took: 226.83095002174377 seconds\n",
      "Epoch [63/75], Train Loss: 5.6595, Test Loss: 5.0678, Top-3 Test Accuracy: 0.3986, took: 174.06125950813293 seconds\n",
      "Epoch [64/75], Train Loss: 5.6568, Test Loss: 5.0583, Top-3 Test Accuracy: 0.3997, took: 175.67416834831238 seconds\n",
      "Epoch [65/75], Train Loss: 5.6559, Test Loss: 5.0579, Top-3 Test Accuracy: 0.4000, took: 173.25627875328064 seconds\n",
      "Epoch [66/75], Train Loss: 5.6512, Test Loss: 5.0578, Top-3 Test Accuracy: 0.3994, took: 170.55566453933716 seconds\n",
      "Epoch [67/75], Train Loss: 5.6501, Test Loss: 5.0574, Top-3 Test Accuracy: 0.3998, took: 173.8964786529541 seconds\n",
      "Epoch [68/75], Train Loss: 5.6445, Test Loss: 5.0508, Top-3 Test Accuracy: 0.4005, took: 233.19274878501892 seconds\n",
      "Epoch [69/75], Train Loss: 5.6445, Test Loss: 5.0520, Top-3 Test Accuracy: 0.4007, took: 173.20734071731567 seconds\n",
      "Epoch [70/75], Train Loss: 5.6403, Test Loss: 5.0433, Top-3 Test Accuracy: 0.4018, took: 174.58358693122864 seconds\n",
      "Epoch [71/75], Train Loss: 5.6413, Test Loss: 5.0573, Top-3 Test Accuracy: 0.3997, took: 172.58774423599243 seconds\n",
      "Epoch [72/75], Train Loss: 5.6332, Test Loss: 5.0466, Top-3 Test Accuracy: 0.4018, took: 172.37110495567322 seconds\n",
      "Epoch [73/75], Train Loss: 5.6390, Test Loss: 5.0449, Top-3 Test Accuracy: 0.4010, took: 227.3388340473175 seconds\n",
      "Epoch [74/75], Train Loss: 5.6342, Test Loss: 5.0492, Top-3 Test Accuracy: 0.4010, took: 172.32887530326843 seconds\n",
      "Epoch [75/75], Train Loss: 5.6286, Test Loss: 5.0372, Top-3 Test Accuracy: 0.4018, took: 172.15411114692688 seconds\n",
      "Finished Training\n",
      "Total Epochs: 75\n",
      "Final Training Loss: 5.6286\n",
      "Final Test Loss: 5.0372\n",
      "Final Test Accuracy: 0.4018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    t = time.time()\n",
    "    # Training\n",
    "    model_nn_noScraping.train()  # Set the model to train mode\n",
    "    total_train_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.view(-1, input_size)\n",
    "        outputs = model_nn_noScraping(inputs)\n",
    "\n",
    "        # Convert labels from tuple to Tensor\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Testing\n",
    "    model_nn_noScraping.eval()  # Set the model to evaluation mode\n",
    "    total_correct_top3 = 0\n",
    "    total_samples = 0\n",
    "    total_test_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.view(-1, input_size)\n",
    "            outputs = model_nn_noScraping(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Convert labels from tuple to Tensor\n",
    "            labels = torch.tensor(labels)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.topk(outputs, k=3, dim=1)  # Get top-3 predictions\n",
    "            total_correct_top3 += sum([label in pred_list for label, pred_list in zip(labels, predicted)])\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            true_labels.extend(labels.numpy())\n",
    "            predicted_labels.extend(predicted.numpy())\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    test_top3_accuracy = total_correct_top3 / total_samples\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Top-3 Test Accuracy: {test_top3_accuracy:.4f}, took: {time.time() - t} seconds')\n",
    "\n",
    "print('Finished Training')\n",
    "print(f'Total Epochs: {num_epochs}')\n",
    "print(f'Final Training Loss: {avg_train_loss:.4f}')\n",
    "print(f'Final Test Loss: {avg_test_loss:.4f}')\n",
    "print(f'Final Test Accuracy: {test_top3_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d87c91-7d48-474f-b203-0d5dfaf57002",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model as a .pkl file\n",
    "import torch\n",
    "name = f\"finalNNNoSrcraping.pkl\"\n",
    "# Specify the file path to save the model\n",
    "model_path = f\"/dbfs/FileStore/{name}\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model_nn_noScraping, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a094cc7b-0fc4-4267-a81d-ab4c519a561d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With the scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047ff7f7-fa6c-45ba-8783-d339d68c52ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622aebed-6f33-4a7e-862c-4f6026d75cce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_israeli = pdf_israeli.copy()\n",
    "pandas_df = pdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa1f77c-1530-475e-ac0c-e97bb8d74508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UniqueLabels_all = pandas_df['label'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(UniqueLabels_all)}\n",
    "pandas_df['label'] = pandas_df['label'].map(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6d8a68-f428-4182-8c2c-2eaa31e35085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UniqueLabels_israeli = pandas_df_israeli['label'].unique()\n",
    "label_to_idx = {label: idx+12668 for idx, label in enumerate(UniqueLabels_israeli)}\n",
    "pandas_df_israeli['label'] = pandas_df_israeli['label'].map(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63e1a3c-b095-4963-a6c4-abe5d8dc8d45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenating the two dataframes\n",
    "concatenated_df = pd.concat([pandas_df, pandas_df_israeli])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af80c551-9b43-4bea-88ff-db59ee9ed323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UniqueLabels= concatenated_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfdcbfb-6e83-4d27-8b00-cba95be5d1f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Splitting the data into features (X) and labels (y)\n",
    "X = np.array(concatenated_df['features'].tolist())\n",
    "y = np.array(concatenated_df['label'])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a68dc27-5b38-404a-9941-290da6c3b15d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 5 | Accuracy: 0.19366395201099748 | Precision: 0.2055844297674794 | Recall: 0.19366395201099748 | F1-score: 0.1889949813777644\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 7 | Accuracy: 0.19830872091812293 | Precision: 0.2064468157542146 | Recall: 0.19830872091812293 | F1-score: 0.1909125194048667\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 9 | Accuracy: 0.20120389077503073 | Precision: 0.20526188004892684 | Recall: 0.20120389077503073 | F1-score: 0.19120562963043342\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 11 | Accuracy: 0.20343254670804609 | Precision: 0.20322031859701187 | Recall: 0.20343254670804609 | F1-score: 0.19063533357319906\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 13 | Accuracy: 0.20489054591656078 | Precision: 0.20078651918073004 | Recall: 0.20489054591656078 | F1-score: 0.189350139375981\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 15 | Accuracy: 0.20651517360604862 | Precision: 0.1994568304166294 | Recall: 0.20651517360604862 | F1-score: 0.18865243371941745\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 17 | Accuracy: 0.20786903001395513 | Precision: 0.19856442101399072 | Recall: 0.20786903001395513 | F1-score: 0.1879446615656279\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 19 | Accuracy: 0.20803565849492825 | Precision: 0.19720565053884023 | Recall: 0.20803565849492825 | F1-score: 0.1861526960068048\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 21 | Accuracy: 0.20901460082064527 | Precision: 0.19636964747896396 | Recall: 0.20901460082064527 | F1-score: 0.18519683021196623\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 23 | Accuracy: 0.20916040074149675 | Precision: 0.1958365525686936 | Recall: 0.20916040074149675 | F1-score: 0.1840183326144506\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 25 | Accuracy: 0.2093062006623482 | Precision: 0.19575173237570245 | Recall: 0.2093062006623482 | F1-score: 0.18302317591347053\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Weights: distance | K: 27 | Accuracy: 0.2099935431463623 | Precision: 0.1958718490128194 | Recall: 0.2099935431463623 | F1-score: 0.18249146837817382\n",
      "Weights: distance | K: 29 | Accuracy: 0.20974360042490262 | Precision: 0.19455407806699485 | Recall: 0.20974360042490262 | F1-score: 0.18105212055962938\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/databricks/python/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for w in [\"distance\"]:\n",
    "    for k in range(5, 30,2):\n",
    "        # Initializing the K-Nearest Neighbors classifier\n",
    "        knn_classifier = KNeighborsClassifier(n_neighbors=k, weights=w)\n",
    "\n",
    "        # Fitting the classifier to the training data\n",
    "        knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Predicting labels for the test set\n",
    "        y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "        # Calculating evaluation metrics\n",
    "        accuracy, precision, recall, f1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted'), recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Printing evaluation metrics\n",
    "        print(\"Weights:\", w, \"| K:\", k, \"| Accuracy:\", accuracy, \"| Precision:\", precision, \"| Recall:\", recall, \"| F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614db4d0-7d19-44ad-b9e2-fa91581a7d93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ea2ce3-5bc2-4ea3-b8c4-81051e2ec23d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network architecture\n",
    "class ComplexNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.75):\n",
    "        super(ComplexNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        nn.Tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = np.array(self.df.iloc[idx]['features'], dtype=np.float32)\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return features, label\n",
    "    \n",
    "\n",
    "outputLayer = len(UniqueLabels)\n",
    "# Hyperparameters\n",
    "input_size = 128\n",
    "hidden_size1 = 512\n",
    "hidden_size2 = 512\n",
    "output_size = len(UniqueLabels)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 75\n",
    "batch_size = 64\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "grouped_df = concatenated_df.groupby('label')\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for label, group in grouped_df:\n",
    "    train_group, test_group = train_test_split(group, test_size=0.2, random_state=42)\n",
    "    train_data.append(train_group)\n",
    "    test_data.append(test_group)\n",
    "\n",
    "train_df = pd.concat(train_data)\n",
    "test_df = pd.concat(test_data)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Create dataset and dataloader for train and test sets\n",
    "train_dataset = CustomDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_df)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ComplexNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d549db-733b-48d7-8952-3caa4602f8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<command-3791678015590403>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n",
      "<command-3791678015590403>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n",
      "Epoch [1/75], Train Loss: 8.2385, Test Loss: 7.1234, Top-3 Test Accuracy: 0.1862, took: 202.51670932769775 seconds\n",
      "Epoch [2/75], Train Loss: 7.1894, Test Loss: 6.4977, Top-3 Test Accuracy: 0.2443, took: 208.6511845588684 seconds\n",
      "Epoch [3/75], Train Loss: 6.7791, Test Loss: 6.1806, Top-3 Test Accuracy: 0.2777, took: 253.9337182044983 seconds\n",
      "Epoch [4/75], Train Loss: 6.5691, Test Loss: 6.0123, Top-3 Test Accuracy: 0.2942, took: 242.70818901062012 seconds\n",
      "Epoch [5/75], Train Loss: 6.4376, Test Loss: 5.8900, Top-3 Test Accuracy: 0.3095, took: 237.8256380558014 seconds\n",
      "Epoch [6/75], Train Loss: 6.3429, Test Loss: 5.7934, Top-3 Test Accuracy: 0.3193, took: 212.90920853614807 seconds\n",
      "Epoch [7/75], Train Loss: 6.2683, Test Loss: 5.7193, Top-3 Test Accuracy: 0.3279, took: 237.38962507247925 seconds\n",
      "Epoch [8/75], Train Loss: 6.2200, Test Loss: 5.6660, Top-3 Test Accuracy: 0.3343, took: 177.56411790847778 seconds\n",
      "Epoch [9/75], Train Loss: 6.1713, Test Loss: 5.6176, Top-3 Test Accuracy: 0.3384, took: 176.9328315258026 seconds\n",
      "Epoch [10/75], Train Loss: 6.1336, Test Loss: 5.5938, Top-3 Test Accuracy: 0.3416, took: 177.81889128684998 seconds\n",
      "Epoch [11/75], Train Loss: 6.1020, Test Loss: 5.5607, Top-3 Test Accuracy: 0.3464, took: 178.33262848854065 seconds\n",
      "Epoch [12/75], Train Loss: 6.0812, Test Loss: 5.5291, Top-3 Test Accuracy: 0.3488, took: 240.6109094619751 seconds\n",
      "Epoch [13/75], Train Loss: 6.0534, Test Loss: 5.5076, Top-3 Test Accuracy: 0.3504, took: 224.68105816841125 seconds\n",
      "Epoch [14/75], Train Loss: 6.0269, Test Loss: 5.4824, Top-3 Test Accuracy: 0.3534, took: 230.9638671875 seconds\n",
      "Epoch [15/75], Train Loss: 6.0098, Test Loss: 5.4508, Top-3 Test Accuracy: 0.3560, took: 177.89713382720947 seconds\n",
      "Epoch [16/75], Train Loss: 5.9899, Test Loss: 5.4351, Top-3 Test Accuracy: 0.3584, took: 180.32333612442017 seconds\n",
      "Epoch [17/75], Train Loss: 5.9684, Test Loss: 5.4278, Top-3 Test Accuracy: 0.3611, took: 227.9579463005066 seconds\n",
      "Epoch [18/75], Train Loss: 5.9540, Test Loss: 5.3917, Top-3 Test Accuracy: 0.3621, took: 176.21728324890137 seconds\n",
      "Epoch [19/75], Train Loss: 5.9408, Test Loss: 5.3884, Top-3 Test Accuracy: 0.3636, took: 177.31416296958923 seconds\n",
      "Epoch [20/75], Train Loss: 5.9233, Test Loss: 5.3681, Top-3 Test Accuracy: 0.3670, took: 178.1321861743927 seconds\n",
      "Epoch [21/75], Train Loss: 5.9073, Test Loss: 5.3637, Top-3 Test Accuracy: 0.3654, took: 177.76760816574097 seconds\n",
      "Epoch [22/75], Train Loss: 5.9001, Test Loss: 5.3378, Top-3 Test Accuracy: 0.3688, took: 178.73725986480713 seconds\n",
      "Epoch [23/75], Train Loss: 5.8886, Test Loss: 5.3138, Top-3 Test Accuracy: 0.3716, took: 177.8949899673462 seconds\n",
      "Epoch [24/75], Train Loss: 5.8815, Test Loss: 5.3097, Top-3 Test Accuracy: 0.3705, took: 179.15089082717896 seconds\n",
      "Epoch [25/75], Train Loss: 5.8665, Test Loss: 5.2883, Top-3 Test Accuracy: 0.3741, took: 177.61243271827698 seconds\n",
      "Epoch [26/75], Train Loss: 5.8654, Test Loss: 5.2985, Top-3 Test Accuracy: 0.3720, took: 176.30264139175415 seconds\n",
      "Epoch [27/75], Train Loss: 5.8560, Test Loss: 5.2809, Top-3 Test Accuracy: 0.3734, took: 176.9836549758911 seconds\n",
      "Epoch [28/75], Train Loss: 5.8412, Test Loss: 5.2861, Top-3 Test Accuracy: 0.3754, took: 175.8693389892578 seconds\n",
      "Epoch [29/75], Train Loss: 5.8337, Test Loss: 5.2820, Top-3 Test Accuracy: 0.3750, took: 177.81676030158997 seconds\n",
      "Epoch [30/75], Train Loss: 5.8269, Test Loss: 5.2674, Top-3 Test Accuracy: 0.3765, took: 242.25870275497437 seconds\n",
      "Epoch [31/75], Train Loss: 5.8228, Test Loss: 5.2539, Top-3 Test Accuracy: 0.3779, took: 176.37678909301758 seconds\n",
      "Epoch [32/75], Train Loss: 5.8118, Test Loss: 5.2347, Top-3 Test Accuracy: 0.3797, took: 236.7199728488922 seconds\n",
      "Epoch [33/75], Train Loss: 5.8070, Test Loss: 5.2201, Top-3 Test Accuracy: 0.3812, took: 233.66139960289001 seconds\n",
      "Epoch [34/75], Train Loss: 5.7979, Test Loss: 5.2301, Top-3 Test Accuracy: 0.3819, took: 176.90718698501587 seconds\n",
      "Epoch [35/75], Train Loss: 5.7917, Test Loss: 5.2217, Top-3 Test Accuracy: 0.3818, took: 177.33140802383423 seconds\n",
      "Epoch [36/75], Train Loss: 5.7867, Test Loss: 5.1890, Top-3 Test Accuracy: 0.3842, took: 176.0613763332367 seconds\n",
      "Epoch [37/75], Train Loss: 5.7808, Test Loss: 5.2014, Top-3 Test Accuracy: 0.3858, took: 176.70530605316162 seconds\n",
      "Epoch [38/75], Train Loss: 5.7726, Test Loss: 5.1836, Top-3 Test Accuracy: 0.3834, took: 176.61157250404358 seconds\n",
      "Epoch [39/75], Train Loss: 5.7733, Test Loss: 5.1869, Top-3 Test Accuracy: 0.3850, took: 177.52037978172302 seconds\n",
      "Epoch [40/75], Train Loss: 5.7675, Test Loss: 5.1799, Top-3 Test Accuracy: 0.3866, took: 223.85786604881287 seconds\n",
      "Epoch [41/75], Train Loss: 5.7605, Test Loss: 5.1981, Top-3 Test Accuracy: 0.3848, took: 178.39840865135193 seconds\n",
      "Epoch [42/75], Train Loss: 5.7545, Test Loss: 5.1764, Top-3 Test Accuracy: 0.3876, took: 176.05813312530518 seconds\n",
      "Epoch [43/75], Train Loss: 5.7514, Test Loss: 5.1566, Top-3 Test Accuracy: 0.3873, took: 236.77435040473938 seconds\n",
      "Epoch [44/75], Train Loss: 5.7483, Test Loss: 5.1581, Top-3 Test Accuracy: 0.3881, took: 235.77719807624817 seconds\n",
      "Epoch [45/75], Train Loss: 5.7412, Test Loss: 5.1411, Top-3 Test Accuracy: 0.3902, took: 218.26054167747498 seconds\n",
      "Epoch [46/75], Train Loss: 5.7406, Test Loss: 5.1543, Top-3 Test Accuracy: 0.3884, took: 176.38522291183472 seconds\n",
      "Epoch [47/75], Train Loss: 5.7325, Test Loss: 5.1504, Top-3 Test Accuracy: 0.3919, took: 177.2083683013916 seconds\n",
      "Epoch [48/75], Train Loss: 5.7304, Test Loss: 5.1538, Top-3 Test Accuracy: 0.3876, took: 178.35162329673767 seconds\n",
      "Epoch [49/75], Train Loss: 5.7256, Test Loss: 5.1396, Top-3 Test Accuracy: 0.3903, took: 236.6585443019867 seconds\n",
      "Epoch [50/75], Train Loss: 5.7230, Test Loss: 5.1464, Top-3 Test Accuracy: 0.3893, took: 176.2984380722046 seconds\n",
      "Epoch [51/75], Train Loss: 5.7117, Test Loss: 5.1324, Top-3 Test Accuracy: 0.3926, took: 176.2713975906372 seconds\n",
      "Epoch [52/75], Train Loss: 5.7149, Test Loss: 5.1291, Top-3 Test Accuracy: 0.3913, took: 177.4565269947052 seconds\n",
      "Epoch [53/75], Train Loss: 5.7095, Test Loss: 5.1148, Top-3 Test Accuracy: 0.3946, took: 175.70741152763367 seconds\n",
      "Epoch [54/75], Train Loss: 5.7013, Test Loss: 5.1019, Top-3 Test Accuracy: 0.3959, took: 175.5674901008606 seconds\n",
      "Epoch [55/75], Train Loss: 5.7035, Test Loss: 5.1155, Top-3 Test Accuracy: 0.3943, took: 233.38591074943542 seconds\n",
      "Epoch [56/75], Train Loss: 5.6985, Test Loss: 5.1154, Top-3 Test Accuracy: 0.3949, took: 176.08828330039978 seconds\n",
      "Epoch [57/75], Train Loss: 5.6950, Test Loss: 5.1036, Top-3 Test Accuracy: 0.3959, took: 176.79838347434998 seconds\n",
      "Epoch [58/75], Train Loss: 5.6999, Test Loss: 5.1081, Top-3 Test Accuracy: 0.3958, took: 177.69262075424194 seconds\n",
      "Epoch [59/75], Train Loss: 5.6875, Test Loss: 5.1027, Top-3 Test Accuracy: 0.3935, took: 201.89810347557068 seconds\n",
      "Epoch [60/75], Train Loss: 5.6842, Test Loss: 5.0927, Top-3 Test Accuracy: 0.3955, took: 175.81775212287903 seconds\n",
      "Epoch [61/75], Train Loss: 5.6860, Test Loss: 5.0954, Top-3 Test Accuracy: 0.3966, took: 236.59497261047363 seconds\n",
      "Epoch [62/75], Train Loss: 5.6790, Test Loss: 5.0828, Top-3 Test Accuracy: 0.3981, took: 177.69725513458252 seconds\n",
      "Epoch [63/75], Train Loss: 5.6795, Test Loss: 5.0741, Top-3 Test Accuracy: 0.3964, took: 233.30695390701294 seconds\n",
      "Epoch [64/75], Train Loss: 5.6762, Test Loss: 5.0782, Top-3 Test Accuracy: 0.3984, took: 177.30498886108398 seconds\n",
      "Epoch [65/75], Train Loss: 5.6744, Test Loss: 5.0778, Top-3 Test Accuracy: 0.3972, took: 176.08520364761353 seconds\n",
      "Epoch [66/75], Train Loss: 5.6686, Test Loss: 5.0702, Top-3 Test Accuracy: 0.3999, took: 203.91614842414856 seconds\n",
      "Epoch [67/75], Train Loss: 5.6655, Test Loss: 5.0854, Top-3 Test Accuracy: 0.3996, took: 176.74788689613342 seconds\n",
      "Epoch [68/75], Train Loss: 5.6696, Test Loss: 5.0718, Top-3 Test Accuracy: 0.3993, took: 177.16065669059753 seconds\n",
      "Epoch [69/75], Train Loss: 5.6638, Test Loss: 5.0665, Top-3 Test Accuracy: 0.3997, took: 176.50189304351807 seconds\n",
      "Epoch [70/75], Train Loss: 5.6620, Test Loss: 5.0608, Top-3 Test Accuracy: 0.3981, took: 176.6335802078247 seconds\n",
      "Epoch [71/75], Train Loss: 5.6612, Test Loss: 5.0586, Top-3 Test Accuracy: 0.3999, took: 177.75296783447266 seconds\n",
      "Epoch [72/75], Train Loss: 5.6571, Test Loss: 5.0611, Top-3 Test Accuracy: 0.4017, took: 175.55829191207886 seconds\n",
      "Epoch [73/75], Train Loss: 5.6554, Test Loss: 5.0538, Top-3 Test Accuracy: 0.4008, took: 177.59862303733826 seconds\n",
      "Epoch [74/75], Train Loss: 5.6524, Test Loss: 5.0781, Top-3 Test Accuracy: 0.4009, took: 177.01051449775696 seconds\n",
      "Epoch [75/75], Train Loss: 5.6514, Test Loss: 5.0494, Top-3 Test Accuracy: 0.4019, took: 176.8531084060669 seconds\n",
      "Finished Training\n",
      "Total Epochs: 75\n",
      "Final Training Loss: 5.6514\n",
      "Final Test Loss: 5.0494\n",
      "Final Test Accuracy: 0.4019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    t = time.time()\n",
    "    # Training\n",
    "    model.train()  # Set the model to train mode\n",
    "    total_train_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.view(-1, input_size)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Convert labels from tuple to Tensor\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct_top3 = 0\n",
    "    total_samples = 0\n",
    "    total_test_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.view(-1, input_size)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Convert labels from tuple to Tensor\n",
    "            labels = torch.tensor(labels)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.topk(outputs, k=3, dim=1)  # Get top-3 predictions\n",
    "            total_correct_top3 += sum([label in pred_list for label, pred_list in zip(labels, predicted)])\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            true_labels.extend(labels.numpy())\n",
    "            predicted_labels.extend(predicted.numpy())\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    test_top3_accuracy = total_correct_top3 / total_samples\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Top-3 Test Accuracy: {test_top3_accuracy:.4f}, took: {time.time() - t} seconds')\n",
    "\n",
    "print('Finished Training')\n",
    "print(f'Total Epochs: {num_epochs}')\n",
    "print(f'Final Training Loss: {avg_train_loss:.4f}')\n",
    "print(f'Final Test Loss: {avg_test_loss:.4f}')\n",
    "print(f'Final Test Accuracy: {test_top3_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a016680-38c6-4c15-8127-2e320607806a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model as a .pkl file\n",
    "import torch\n",
    "name = f\"finalNNWithSrcraping.pkl\"\n",
    "# Specify the file path to save the model\n",
    "model_path = f\"/dbfs/FileStore/{name}\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a284be7-f2f6-4465-87f2-c52786cdd35a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Checking the average accuracy of the model only on the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16c8cbc-d72d-4709-810a-2102726cf70b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the scraped data in test file.\n",
    "scrapedTestDF = test_df[test_df['label'] >= 12668].sort_values(by='label', ascending=True).copy()\n",
    "scrapedTest_dataset = CustomDataset(scrapedTestDF)\n",
    "scrapedTest_dataloader = DataLoader(scrapedTest_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93646c19-6b39-46ac-a88b-0cdcbd27c667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[105]: 168"
     ]
    }
   ],
   "source": [
    "scrapedTestDF[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f896efd2-001b-4737-9cc9-6a11d94e7e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.1974, Top-3 Test Accuracy: 0.4949\n",
      "<command-3791678015591356>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct_top3 = 0\n",
    "total_samples = 0\n",
    "total_test_loss = 0.0\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in scrapedTest_dataloader:\n",
    "        inputs = inputs.view(-1, input_size)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Convert labels from tuple to Tensor\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.topk(outputs, k=3, dim=1)  # Get top-3 predictions\n",
    "        total_correct_top3 += sum([label in pred_list for label, pred_list in zip(labels, predicted)])\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "\n",
    "avg_test_loss = total_test_loss / len(scrapedTest_dataloader)\n",
    "test_top3_accuracy = total_correct_top3 / total_samples\n",
    "\n",
    "print(f'Test Loss: {avg_test_loss:.4f}, Top-3 Test Accuracy: {test_top3_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3791678015590262,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
